# CSC620 - HA3: Text Classification Using Naive Bayes

This assignment has two parts:

- **Part 1:** I implement a Naive Bayes text classifier completely from scratch (no scikit-learn).
- **Part 2:** I train and evaluate a Naive Bayes classifier using scikit-learn on a real sarcasm detection dataset.

Both parts are organized in separate folders so it’s easy to run and grade.


## Repository Structure

```text
HA3-CSC620/
├─ README.md
│
├─ part1/
│  ├─ train.csv
│  ├─ test.csv
│  ├─ nb_train.py
│  ├─ model.csv
│  ├─ nb_test.py
│  ├─ test_predictions.csv
│
└─ part2/
   ├─ nb_analysis.ipynb
   ├─ Sarcasm_Headlines_Dataset.json
   └─ Sarcasm_Headlines_Dataset_v2.json
```


---

## Part 1: From-Scratch Naive Bayes

### Goal
Build a simple text classifier with two classes (ex: `spam` vs `ham`) using my own code:
- estimate priors `P(class)`
- estimate likelihoods `P(word | class)` with Laplace smoothing
- classify new messages by choosing the class with the highest score

No external ML libraries are used here — it’s all manual math.


### Files in `part1_basic_nb/`

#### `train.csv`
- My training data.
- Around 80% of the total dataset.
- Format: two columns with a header:
  ```csv
  text,label
  "win a free iphone click this link now",spam
  "hey are we still meeting at 5",ham
  ...
  ```

#### `test.csv`
- My held-out test data.
- Around 20% of the dataset.
- Same format (`text,label`).

#### `nb_train.py`
- Trains Naive Bayes from scratch.
- Steps:
  1. Reads `train.csv`
  2. Calculates:
     - Prior probabilities `P(class)` = (#docs in class) / (total docs)
     - Likelihood probabilities `P(word|class)` with Laplace smoothing:  
       \[
       P(w|c) = \frac{\text{count}(w,c) + 1}{\text{total words in }c + V}
       \]  
       where `V` is vocabulary size
  3. Writes the learned model to `model.csv`

- Output format of `model.csv` matches assignment spec:
  ```text
  PP    spam,0.5  ham,0.5
  LP    win,spam,0.0123  win,ham,0.0045  ...
  ```

  - `PP` = prior probabilities  
  - `LP` = likelihoods of words per class


#### `model.csv`
- Generated by `nb_train.py`.
- Contains:
  - priors for each class
  - likelihood of every word in each class
- This file is what the test script will use to classify new messages.


#### `nb_test.py`
- Uses the trained model to make predictions on new text.
- Steps:
  1. Loads `model.csv` (priors + likelihoods)
  2. Loads `test.csv`
  3. For each row in `test.csv`, predicts a label using Naive Bayes:  
     \[
     \text{score}(c) = \log P(c) + \sum_{w \in \text{doc}} \log P(w|c)
     \]  
     (We use log probs to avoid underflow.)
  4. Writes results to `test_predictions.csv`

- For words that never appeared in training for that class, the code uses a tiny fallback probability (`1e-10`) to avoid `log(0)` crashes.


#### `test_predictions.csv`
- Generated by `nb_test.py`.
- Format:
  ```csv
  text,predicted,actual
  "final notice your warranty expired click now",spam,spam
  "professor said exam is moved to monday",ham,ham
  ...
  ```

- This is the evaluation output for Part 1.


### How to Run Part 1

From inside `part1/`:

1. Train the model
   ```bash
   python3 nb_train.py
   ```
   This creates/updates `model.csv`.

2. Test / get predictions
   ```bash
   python3 nb_test.py
   ```
   This creates/updates `test_predictions.csv`.

You will submit all of these files in Part 1.


---

## Part 2: Naive Bayes with scikit-learn (Sarcasm Detection)

### Goal
Use a real-world dataset and scikit-learn’s `MultinomialNB` to:
- vectorize text using bag-of-words
- train a Naive Bayes classifier
- evaluate using accuracy, precision, recall, f1-score, confusion matrix
- explain the results
- compare this to Part 1

This follows the standard sarcasm detection approach with Naive Bayes and bag-of-words.


### Files in `part2/`

#### `Sarcasm_Headlines_Dataset.json`
- Dataset with news headlines.
- Each row has (at least):
  - `"headline"`: the text
  - `"is_sarcastic"`: 1 = sarcastic, 0 = not sarcastic
- This is the dataset loaded by the notebook.

(If you also have `Sarcasm_Headlines_Dataset_v2.json`, include it here too. The notebook just needs to read the one you’re actually using.)


#### `nb_analysis.ipynb`
This is the notebook for Part 2. It contains:

1. Intro / context  
   A short description of the task: sarcasm detection, Naive Bayes, what we’ll measure.

2. Imports  
   ```python
   import pandas as pd
   from sklearn.model_selection import train_test_split
   from sklearn.feature_extraction.text import CountVectorizer
   from sklearn.naive_bayes import MultinomialNB
   from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
   ```

3. Load dataset  
   ```python
   data = pd.read_json("Sarcasm_Headlines_Dataset.json", lines=True)
   ```
   Then show:
   - the first few rows
   - total number of rows

4. Label distribution / examples  
   - How many sarcastic vs non-sarcastic
   - Some example sarcastic headlines and some non-sarcastic headlines

5. Train/test split (80/20)  
   ```python
   X = data["headline"]
   y = data["is_sarcastic"]

   X_train, X_test, y_train, y_test = train_test_split(
       X, y, test_size=0.2, random_state=42, stratify=y
   )
   ```

6. Vectorization with CountVectorizer  
   - Converts text into bag-of-words counts  
   - This is basically the automated version of what I did manually in Part 1

7. Train Naive Bayes (`MultinomialNB`)  
   - Fits Naive Bayes on the training vectors  
   - Learns priors `P(class)` and word likelihoods `P(word|class)` internally

8. Evaluation  
   On the test set, the notebook prints:
   - `accuracy_score`
   - `classification_report` (precision, recall, f1-score, support)
   - `confusion_matrix`

9. Metric explanation (printed in the notebook)  
   I explain in plain English:
   - precision = how often we’re right when we predict a class  
   - recall = how much of that class we successfully catch  
   - f1-score = balance between precision and recall  
   - support = how many samples of that class exist  
   - accuracy = overall percent correct  
   - confusion matrix layout and what each cell means

   This directly covers the requirement:
   “Explain in detail what output the .classification_report() function provides and how it measures classification performance.”

10. Part 1 vs Part 2 comparison (printed in the notebook)  
    I compare:
    - small hand-built spam/ham dataset vs large sarcasm dataset
    - manual counting + smoothing vs CountVectorizer + MultinomialNB
    - basic accuracy check in Part 1 vs full classification_report / confusion_matrix in Part 2
    - handling unseen words by hand vs done automatically by scikit-learn
    - why Part 2 is closer to a real-world text classification workflow

    This covers:
    “Compare the performance of the Naive Bayes classifier from Part 1 and Part 2… Identify 3 to 5 key insights.”


### How to Run Part 2

Part 2 uses scikit-learn, which is not always installed system-wide on macOS.  
Recommended: use a virtual environment.

From the root of the project (`HA3_NaiveBayes/`):

```bash
# create a virtual environment
python3 -m venv venv

# activate it
source venv/bin/activate

# install needed packages into this isolated env
pip install pandas scikit-learn notebook
```

Now run Jupyter from inside that environment:

```bash
cd part2_sklearn_nb
jupyter notebook
```

In the Jupyter UI:
1. Open `nb_analysis.ipynb`
2. Run all cells (top to bottom)
3. Make sure the output cells (accuracy, classification report, confusion matrix, explanations) are visible
4. Save the notebook


### Expected notebook output
By the end of `nb_analysis.ipynb`, we should see:
- how many sarcastic vs non-sarcastic headlines there are,
- the model’s accuracy,
- the `classification_report()` table,
- the confusion matrix,
- printed explanation of the metrics,
- printed comparison of Part 1 vs Part 2.


---

## What to Submit

**Part 1 (`part1_/`):**
- `train.csv`
- `test.csv`
- `nb_train.py`
- `model.csv` (generated by `nb_train.py`)
- `nb_test.py`
- `test_predictions.csv` (generated by `nb_test.py`)

**Part 2 (`part2/`):**
- `nb_analysis.ipynb` (with all cells already run so the outputs are saved in the notebook)
- `Sarcasm_Headlines_Dataset.json` (the dataset that the notebook loads)

You do **not** need to submit the `venv/` folder.


---

## Notes for the Grader / Instructor

- Part 1 shows the Naive Bayes math implemented manually:
  - tokenizing text
  - counting word frequencies per class
  - computing smoothed likelihoods
  - generating a model file (`model.csv`)
  - classifying test messages and writing out predictions

- Part 2 shows scaling that same idea to a real dataset with scikit-learn:
  - bag-of-words via `CountVectorizer`
  - `MultinomialNB` to learn the same probability model
  - formal evaluation using precision / recall / f1-score / accuracy
  - confusion matrix analysis
  - written interpretation of results
  - a direct comparison of Part 1 vs Part 2 (performance, robustness, eval quality)
