{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CSC620 – Naive Bayes Text Classification (Sarcasm Detection)\n",
      "\n",
      "Goal:\n",
      "- Train and evaluate a Naive Bayes text classifier using scikit-learn.\n",
      "- Dataset: News Headlines Dataset for Sarcasm Detection.\n",
      "  'is_sarcastic' = 1 means sarcastic, 0 means not sarcastic.\n",
      "- Model: Multinomial Naive Bayes (bag-of-words).\n",
      "\n",
      "At the end we will:\n",
      "1. Show accuracy, precision, recall, f1-score, and confusion matrix.\n",
      "2. Explain what those metrics mean.\n",
      "3. Compare this model to my custom Naive Bayes from Part 1.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "intro_text = \"\"\"\n",
    "CSC620 – Naive Bayes Text Classification (Sarcasm Detection)\n",
    "\n",
    "Goal:\n",
    "- Train and evaluate a Naive Bayes text classifier using scikit-learn.\n",
    "- Dataset: News Headlines Dataset for Sarcasm Detection.\n",
    "  'is_sarcastic' = 1 means sarcastic, 0 means not sarcastic.\n",
    "- Model: Multinomial Naive Bayes (bag-of-words).\n",
    "\n",
    "At the end we will:\n",
    "1. Show accuracy, precision, recall, f1-score, and confusion matrix.\n",
    "2. Explain what those metrics mean.\n",
    "3. Compare this model to my custom Naive Bayes from Part 1.\n",
    "\"\"\"\n",
    "\n",
    "print(intro_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries we need:\n",
    "# - pandas: load and inspect the dataset\n",
    "# - train_test_split: split data into train and test sets\n",
    "# - CountVectorizer: convert text -> word count features\n",
    "# - MultinomialNB: Naive Bayes model for word counts\n",
    "# - metrics: evaluate predictions\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in dataset: ['article_link', 'headline', 'is_sarcastic']\n",
      "\n",
      "First 5 rows:\n",
      "                                        article_link  \\\n",
      "0  https://www.huffingtonpost.com/entry/versace-b...   \n",
      "1  https://www.huffingtonpost.com/entry/roseanne-...   \n",
      "2  https://local.theonion.com/mom-starting-to-fea...   \n",
      "3  https://politics.theonion.com/boehner-just-wan...   \n",
      "4  https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
      "\n",
      "                                            headline  is_sarcastic  \n",
      "0  former versace store clerk sues over secret 'b...             0  \n",
      "1  the 'roseanne' revival catches up to our thorn...             0  \n",
      "2  mom starting to fear son's web series closest ...             1  \n",
      "3  boehner just wants wife to listen, not come up...             1  \n",
      "4  j.k. rowling wishes snape happy birthday in th...             0  \n",
      "\n",
      "Total number of rows: 26709\n"
     ]
    }
   ],
   "source": [
    "# Load the sarcasm dataset. It's a JSON-lines file (one json object per line).\n",
    "# Columns we care about:\n",
    "# - \"headline\": the text we classify\n",
    "# - \"is_sarcastic\": the label (1 sarcastic, 0 not sarcastic)\n",
    "\n",
    "data = pd.read_json(\"Sarcasm_Headlines_Dataset.json\", lines=True)\n",
    "\n",
    "print(\"Columns in dataset:\", data.columns.tolist())\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(data.head())\n",
    "\n",
    "print(\"\\nTotal number of rows:\", len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts:\n",
      " is_sarcastic\n",
      "0    14985\n",
      "1    11724\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sarcastic: 43.90%\n",
      "Not sarcastic: 56.10%\n",
      "\n",
      "Sample sarcastic headlines (is_sarcastic = 1):\n",
      "[\"mom starting to fear son's web series closest thing she will have to grandchild\", 'boehner just wants wife to listen, not come up with alternative debt-reduction ideas', 'top snake handler leaves sinking huckabee campaign', \"nuclear bomb detonates during rehearsal for 'spider-man' musical\", \"cosby lawyer asks why accusers didn't come forward to be smeared by legal team years ago\"]\n",
      "\n",
      "Sample non-sarcastic headlines (is_sarcastic = 0):\n",
      "[\"former versace store clerk sues over secret 'black code' for minority shoppers\", \"the 'roseanne' revival catches up to our thorny political mood, for better and worse\", 'j.k. rowling wishes snape happy birthday in the most magical way', \"advancing the world's women\", 'the fascinating case for eating lab-grown meat']\n"
     ]
    }
   ],
   "source": [
    "# We want to see how many sarcastic vs non-sarcastic headlines there are.\n",
    "# This gives us an idea of class balance, which affects priors in Naive Bayes.\n",
    "\n",
    "label_counts = data[\"is_sarcastic\"].value_counts()\n",
    "print(\"Label counts:\\n\", label_counts)\n",
    "\n",
    "sarcastic_pct = (label_counts[1] / len(data)) * 100\n",
    "nonsarcastic_pct = (label_counts[0] / len(data)) * 100\n",
    "print(f\"\\nSarcastic: {sarcastic_pct:.2f}%\")\n",
    "print(f\"Not sarcastic: {nonsarcastic_pct:.2f}%\")\n",
    "\n",
    "print(\"\\nSample sarcastic headlines (is_sarcastic = 1):\")\n",
    "print(data[data[\"is_sarcastic\"] == 1][\"headline\"].head(5).tolist())\n",
    "\n",
    "print(\"\\nSample non-sarcastic headlines (is_sarcastic = 0):\")\n",
    "print(data[data[\"is_sarcastic\"] == 0][\"headline\"].head(5).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 21367\n",
      "Test size: 5342\n"
     ]
    }
   ],
   "source": [
    "# X = text (headline)\n",
    "# y = label (is_sarcastic)\n",
    "#\n",
    "# We will split 80% train / 20% test.\n",
    "# stratify=y keeps the class ratio similar in both splits.\n",
    "# random_state is for reproducibility.\n",
    "\n",
    "X = data[\"headline\"]\n",
    "y = data[\"is_sarcastic\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Test size:\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 22636\n",
      "Training matrix shape: (21367, 22636)\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes works on counts of words.\n",
    "# CountVectorizer:\n",
    "#   1. learns a vocabulary from the training text\n",
    "#   2. turns each headline into a sparse vector of word counts\n",
    "#\n",
    "# This is basically doing automatically what I did by hand in Part 1\n",
    "# (count words per class, estimate P(word|class)).\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Learn vocabulary and transform training data\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform test data using the same vocabulary\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "print(\"Vocabulary size:\", len(vectorizer.vocabulary_))\n",
    "print(\"Training matrix shape:\", X_train_vec.shape)  # (num_train_examples, num_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes (0 means not sarcastic, 1 means sarcastic): [0 1]\n",
      "Class log priors (log P(class)): [-0.57794153 -0.82337453]\n"
     ]
    }
   ],
   "source": [
    "# MultinomialNB = Multinomial Naive Bayes.\n",
    "# It learns:\n",
    "#   P(class = c)    -> prior\n",
    "#   P(word|class)   -> likelihood of each word in each class\n",
    "#\n",
    "# Basically same math as my Part 1 code, just optimized and built-in.\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_vec, y_train)\n",
    "\n",
    "print(\"Classes (0 means not sarcastic, 1 means sarcastic):\", nb.classes_)\n",
    "print(\"Class log priors (log P(class)):\", nb.class_log_prior_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.8049419692998877\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8114    0.8498    0.8302      2997\n",
      "           1     0.7957    0.7475    0.7709      2345\n",
      "\n",
      "    accuracy                         0.8049      5342\n",
      "   macro avg     0.8036    0.7987    0.8005      5342\n",
      "weighted avg     0.8045    0.8049    0.8042      5342\n",
      "\n",
      "Confusion Matrix:\n",
      " [[2547  450]\n",
      " [ 592 1753]]\n"
     ]
    }
   ],
   "source": [
    "# Let's evaluate on the test set that the model never saw.\n",
    "# We measure:\n",
    "# - accuracy overall\n",
    "# - classification_report (precision, recall, f1-score per class)\n",
    "# - confusion_matrix\n",
    "\n",
    "y_pred = nb.predict(X_test_vec)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, digits=4)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy on test set:\", acc)\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "How to read classification_report():\n",
      "\n",
      "For each class (0 = not sarcastic, 1 = sarcastic):\n",
      "- precision:\n",
      "    Of all headlines the model PREDICTED as this class, how many were actually that class?\n",
      "    High precision for class 1 = when we say 'sarcastic', we're usually right (few false alarms).\n",
      "- recall:\n",
      "    Of all the TRUE headlines of this class in the test set, how many did we correctly find?\n",
      "    High recall for class 1 = we are catching most sarcastic headlines instead of missing them.\n",
      "- f1-score:\n",
      "    Harmonic mean of precision and recall. Balances 'don't lie' vs 'don't miss'.\n",
      "- support:\n",
      "    How many examples of that class were actually in the test set.\n",
      "\n",
      "Bottom of the report:\n",
      "- accuracy:\n",
      "    Overall percent of correct predictions.\n",
      "- macro avg:\n",
      "    Average of precision/recall/F1 treating both classes equally (good if classes are imbalanced).\n",
      "- weighted avg:\n",
      "    Average weighted by how many examples are in each class.\n",
      "\n",
      "Confusion matrix (2x2 here):\n",
      "Rows = actual class, Cols = predicted class\n",
      "\n",
      "[0,0] top-left  = actual non-sarcastic predicted non-sarcastic (correct)\n",
      "[0,1] top-right = actual non-sarcastic predicted sarcastic (false positive for sarcasm)\n",
      "[1,0] bottom-left = actual sarcastic predicted non-sarcastic (false negative for sarcasm)\n",
      "[1,1] bottom-right = actual sarcastic predicted sarcastic (correct)\n",
      "\n",
      "This tells us what kind of mistakes we make more:\n",
      "- Are we calling normal headlines sarcastic too often?\n",
      "- Or are we missing sarcasm because it's subtle?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explanation_metrics = \"\"\"\n",
    "How to read classification_report():\n",
    "\n",
    "For each class (0 = not sarcastic, 1 = sarcastic):\n",
    "- precision:\n",
    "    Of all headlines the model PREDICTED as this class, how many were actually that class?\n",
    "    High precision for class 1 = when we say 'sarcastic', we're usually right (few false alarms).\n",
    "- recall:\n",
    "    Of all the TRUE headlines of this class in the test set, how many did we correctly find?\n",
    "    High recall for class 1 = we are catching most sarcastic headlines instead of missing them.\n",
    "- f1-score:\n",
    "    Harmonic mean of precision and recall. Balances 'don't lie' vs 'don't miss'.\n",
    "- support:\n",
    "    How many examples of that class were actually in the test set.\n",
    "\n",
    "Bottom of the report:\n",
    "- accuracy:\n",
    "    Overall percent of correct predictions.\n",
    "- macro avg:\n",
    "    Average of precision/recall/F1 treating both classes equally (good if classes are imbalanced).\n",
    "- weighted avg:\n",
    "    Average weighted by how many examples are in each class.\n",
    "\n",
    "Confusion matrix (2x2 here):\n",
    "Rows = actual class, Cols = predicted class\n",
    "\n",
    "[0,0] top-left  = actual non-sarcastic predicted non-sarcastic (correct)\n",
    "[0,1] top-right = actual non-sarcastic predicted sarcastic (false positive for sarcasm)\n",
    "[1,0] bottom-left = actual sarcastic predicted non-sarcastic (false negative for sarcasm)\n",
    "[1,1] bottom-right = actual sarcastic predicted sarcastic (correct)\n",
    "\n",
    "This tells us what kind of mistakes we make more:\n",
    "- Are we calling normal headlines sarcastic too often?\n",
    "- Or are we missing sarcasm because it's subtle?\n",
    "\"\"\"\n",
    "\n",
    "print(explanation_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison: Part 1 (my own Naive Bayes) vs Part 2 (scikit-learn Naive Bayes)\n",
      "\n",
      "1. Data size / difficulty\n",
      "   Part 1:\n",
      "     - Very small, hand-built dataset (like spam vs ham texts).\n",
      "     - Tiny vocabulary. Words like \"free\", \"click\", \"prize\" strongly scream spam.\n",
      "   Part 2:\n",
      "     - Real sarcasm headlines, much bigger dataset.\n",
      "     - Sarcasm is harder than spam because sarcasm depends on tone and humor.\n",
      "     - The model is learning from way more examples, so the probabilities are more reliable.\n",
      "\n",
      "2. Feature extraction\n",
      "   Part 1:\n",
      "     - I manually tokenized with .split(), counted words per class,\n",
      "       and computed P(word|class) with Laplace smoothing.\n",
      "     - I wrote those probabilities to model.csv.\n",
      "   Part 2:\n",
      "     - I used CountVectorizer to build a bag-of-words matrix automatically.\n",
      "     - MultinomialNB learns P(class) and P(word|class) for me.\n",
      "     - It's literally the same math idea, just automated and scalable.\n",
      "\n",
      "3. Evaluation\n",
      "   Part 1:\n",
      "     - I output test_predictions.csv with text, predicted, actual.\n",
      "     - I could get accuracy by counting matches.\n",
      "   Part 2:\n",
      "     - I printed classification_report() which gives precision, recall,\n",
      "       f1-score per class, plus overall accuracy.\n",
      "     - I also printed the confusion matrix.\n",
      "     - This shows which class is harder for the model.\n",
      "\n",
      "4. Handling unseen words\n",
      "   Part 1:\n",
      "     - If a test message had a new word that never showed in training,\n",
      "       I had to manually assign a tiny fallback probability (like 1e-10)\n",
      "       so I wouldn't do log(0).\n",
      "   Part 2:\n",
      "     - MultinomialNB handles smoothing internally and is stable with large vocab.\n",
      "\n",
      "5. Real-world usefulness\n",
      "   Part 1:\n",
      "     - More like proof I understand Naive Bayes math and file I/O.\n",
      "   Part 2:\n",
      "     - This is actually usable for a real NLP task (sarcasm detection),\n",
      "       and we can judge how good it is with real metrics.\n",
      "\n",
      "Summary:\n",
      "Part 1 = I built Naive Bayes myself from scratch.\n",
      "Part 2 = I used the same idea, but with scikit-learn on a real dataset,\n",
      "         and I analyzed performance in a more professional way.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comparison_text = \"\"\"\n",
    "Comparison: Part 1 (my own Naive Bayes) vs Part 2 (scikit-learn Naive Bayes)\n",
    "\n",
    "1. Data size / difficulty\n",
    "   Part 1:\n",
    "     - Very small, hand-built dataset (like spam vs ham texts).\n",
    "     - Tiny vocabulary. Words like \"free\", \"click\", \"prize\" strongly scream spam.\n",
    "   Part 2:\n",
    "     - Real sarcasm headlines, much bigger dataset.\n",
    "     - Sarcasm is harder than spam because sarcasm depends on tone and humor.\n",
    "     - The model is learning from way more examples, so the probabilities are more reliable.\n",
    "\n",
    "2. Feature extraction\n",
    "   Part 1:\n",
    "     - I manually tokenized with .split(), counted words per class,\n",
    "       and computed P(word|class) with Laplace smoothing.\n",
    "     - I wrote those probabilities to model.csv.\n",
    "   Part 2:\n",
    "     - I used CountVectorizer to build a bag-of-words matrix automatically.\n",
    "     - MultinomialNB learns P(class) and P(word|class) for me.\n",
    "     - It's literally the same math idea, just automated and scalable.\n",
    "\n",
    "3. Evaluation\n",
    "   Part 1:\n",
    "     - I output test_predictions.csv with text, predicted, actual.\n",
    "     - I could get accuracy by counting matches.\n",
    "   Part 2:\n",
    "     - I printed classification_report() which gives precision, recall,\n",
    "       f1-score per class, plus overall accuracy.\n",
    "     - I also printed the confusion matrix.\n",
    "     - This shows which class is harder for the model.\n",
    "\n",
    "4. Handling unseen words\n",
    "   Part 1:\n",
    "     - If a test message had a new word that never showed in training,\n",
    "       I had to manually assign a tiny fallback probability (like 1e-10)\n",
    "       so I wouldn't do log(0).\n",
    "   Part 2:\n",
    "     - MultinomialNB handles smoothing internally and is stable with large vocab.\n",
    "\n",
    "5. Real-world usefulness\n",
    "   Part 1:\n",
    "     - More like proof I understand Naive Bayes math and file I/O.\n",
    "   Part 2:\n",
    "     - This is actually usable for a real NLP task (sarcasm detection),\n",
    "       and we can judge how good it is with real metrics.\n",
    "\n",
    "Summary:\n",
    "Part 1 = I built Naive Bayes myself from scratch.\n",
    "Part 2 = I used the same idea, but with scikit-learn on a real dataset,\n",
    "         and I analyzed performance in a more professional way.\n",
    "\"\"\"\n",
    "\n",
    "print(comparison_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
